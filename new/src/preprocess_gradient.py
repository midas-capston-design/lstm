#!/usr/bin/env python3
"""ì „ì²˜ë¦¬ëœ CSV â†’ JSONL ë³€í™˜ (Sliding Window + Gradient Features)"""
import json
import csv
import random
from pathlib import Path
from typing import List, Tuple
import numpy as np
import pywt
from tqdm import tqdm
from multiprocessing import Pool, cpu_count

# Random seed ê³ ì •
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# ì •ê·œí™” ê¸°ì¤€ê°’
BASE_MAG = (-33.0, -15.0, -42.0)
COORD_CENTER = (-44.3, -0.3)
COORD_SCALE = 48.8

def normalize_mag(val: float, base: float) -> float:
    return (val - base) / 10.0

def normalize_coord(x: float, y: float) -> Tuple[float, float]:
    x_norm = (x - COORD_CENTER[0]) / COORD_SCALE
    y_norm = (y - COORD_CENTER[1]) / COORD_SCALE
    return (x_norm, y_norm)

def wavelet_denoise(signal: List[float], wavelet='db4', level=3) -> List[float]:
    """Wavelet denoising"""
    if len(signal) < 2**level:
        return signal
    coeffs = pywt.wavedec(signal, wavelet, level=level)
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(len(signal)))
    denoised_coeffs = [pywt.threshold(c, uthresh, mode='soft') for c in coeffs]
    return pywt.waverec(denoised_coeffs, wavelet).tolist()

def process_file(args):
    """íŒŒì¼ í•˜ë‚˜ ì²˜ë¦¬"""
    file_path, window_size, stride = args

    # CSV ì½ê¸°
    with file_path.open() as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    if len(rows) < window_size:
        return []

    # ì‹ í˜¸ ì¶”ì¶œ ë° ì›¨ì´ë¸Œë › ë””ë…¸ì´ì§•
    magx = [float(row['magx']) for row in rows]
    magy = [float(row['magy']) for row in rows]
    magz = [float(row['magz']) for row in rows]

    magx_denoised = wavelet_denoise(magx)
    magy_denoised = wavelet_denoise(magy)
    magz_denoised = wavelet_denoise(magz)

    # Magnitude ë¯¸ë¦¬ ê³„ì‚°
    magnitudes = [
        np.sqrt(magx_denoised[i]**2 + magy_denoised[i]**2 + magz_denoised[i]**2)
        for i in range(len(rows))
    ]

    # Sliding window ìƒì„±
    samples = []
    for i in range(0, len(rows) - window_size + 1, stride):
        window_rows = rows[i:i + window_size]

        # Features: ì •ê·œí™”ëœ ì„¼ì„œê°’ + Gradient
        features = []
        for j, row in enumerate(window_rows):
            idx = i + j

            # í˜„ìž¬ ì‹œì  ê°’
            mag_x = magx_denoised[idx]
            mag_y = magy_denoised[idx]
            mag_z = magz_denoised[idx]
            magnitude = magnitudes[idx]

            # Gradient ê³„ì‚° (ì²« timestepì€ 0)
            if idx == 0 or j == 0:
                delta_x = 0.0
                delta_y = 0.0
                delta_z = 0.0
                delta_mag = 0.0
            else:
                delta_x = mag_x - magx_denoised[idx - 1]
                delta_y = mag_y - magy_denoised[idx - 1]
                delta_z = mag_z - magz_denoised[idx - 1]
                delta_mag = magnitude - magnitudes[idx - 1]

            # 8 features: ê¸°ì¡´ 4ê°œ + Gradient 4ê°œ
            feature_vec = [
                # ê¸°ì¡´ features
                normalize_mag(mag_x, BASE_MAG[0]),
                normalize_mag(mag_y, BASE_MAG[1]),
                normalize_mag(mag_z, BASE_MAG[2]),
                normalize_mag(magnitude, 0.0),
                # Gradient features (baseline=0)
                normalize_mag(delta_x, 0.0),
                normalize_mag(delta_y, 0.0),
                normalize_mag(delta_z, 0.0),
                normalize_mag(delta_mag, 0.0),
            ]
            features.append(feature_vec)

        # Target: ìœˆë„ìš° ëì ì˜ ì •ê·œí™”ëœ ì¢Œí‘œ
        last_row = window_rows[-1]
        x = float(last_row['x'])
        y = float(last_row['y'])
        x_norm, y_norm = normalize_coord(x, y)

        sample = {
            "features": features,
            "target": [x_norm, y_norm]
        }
        samples.append(sample)

    return samples

def main():
    # ì„¤ì •
    preprocessed_dir = Path("data/preprocessed")
    output_dir = Path("new/data/sliding_grad")
    output_dir.mkdir(exist_ok=True, parents=True)

    window_size = 250
    stride = 50

    print("=" * 80)
    print("ì „ì²˜ë¦¬ëœ CSV â†’ JSONL ë³€í™˜ (Gradient Features)")
    print("=" * 80)
    print(f"ìž…ë ¥ ë””ë ‰í† ë¦¬: {preprocessed_dir}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ìœˆë„ìš° í¬ê¸°: {window_size}")
    print(f"ìŠ¤íŠ¸ë¼ì´ë“œ: {stride}")
    print(f"Features: 8 (MagX/Y/Z, Mag, Î”MagX/Y/Z, Î”Mag)")
    print()

    # ìºì‹±: ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸
    meta_path = output_dir / "meta.json"
    train_path = output_dir / "train.jsonl"
    val_path = output_dir / "val.jsonl"
    test_path = output_dir / "test.jsonl"

    if meta_path.exists() and train_path.exists() and val_path.exists() and test_path.exists():
        try:
            with meta_path.open() as f:
                existing_meta = json.load(f)

            # íŒŒë¼ë¯¸í„° ë¹„êµ
            params_match = (
                existing_meta.get("window_size") == window_size and
                existing_meta.get("stride") == stride and
                existing_meta.get("n_features") == 8
            )

            if params_match:
                print("âœ… ì „ì²˜ë¦¬ê°€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
                print(f"   Train: {existing_meta.get('n_train')}ê°œ ìƒ˜í”Œ")
                print(f"   Val:   {existing_meta.get('n_val')}ê°œ ìƒ˜í”Œ")
                print(f"   Test:  {existing_meta.get('n_test')}ê°œ ìƒ˜í”Œ")
                print()
                print("ðŸ’¡ ê°•ì œë¡œ ìž¬ì‹¤í–‰í•˜ë ¤ë©´ meta.jsonì„ ì‚­ì œí•˜ì„¸ìš”.")
                print("=" * 80)
                return
            else:
                print("âš ï¸  ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ì™€ íŒŒë¼ë¯¸í„°ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ìž¬ì‹¤í–‰í•©ë‹ˆë‹¤.")
                print(f"   ê¸°ì¡´: window_size={existing_meta.get('window_size')}, stride={existing_meta.get('stride')}")
                print(f"   ìš”ì²­: window_size={window_size}, stride={stride}")
                print()
        except (json.JSONDecodeError, KeyError) as e:
            print(f"âš ï¸  ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ìž¬ì‹¤í–‰í•©ë‹ˆë‹¤. ({e})")
            print()

    # ëª¨ë“  CSV íŒŒì¼
    csv_files = sorted(preprocessed_dir.glob("*.csv"))
    print(f"ì´ {len(csv_files)}ê°œ íŒŒì¼ ë°œê²¬")

    # ê²½ë¡œë³„ë¡œ íŒŒì¼ ê·¸ë£¹í™”
    from collections import defaultdict
    routes = defaultdict(list)
    for f in csv_files:
        parts = f.stem.split("_")
        route_key = f"{parts[0]}_{parts[1]}"
        routes[route_key].append(f)

    print(f"ì´ {len(routes)}ê°œ ê²½ë¡œ")
    print()

    # Stratified ë¶„í• : ê° ê²½ë¡œì—ì„œ ë¹„ìœ¨ëŒ€ë¡œ ìƒ˜í”Œë§
    train_files = []
    val_files = []
    test_files = []

    for route_key, files in routes.items():
        random.shuffle(files)
        n = len(files)

        if n == 4:
            # 4ê°œ: Train 2ê°œ, Val 1ê°œ, Test 1ê°œ
            train_files.extend(files[:2])
            val_files.append(files[2])
            test_files.append(files[3])
        elif n == 5:
            # 5ê°œ: Train 3ê°œ, Val 1ê°œ, Test 1ê°œ
            train_files.extend(files[:3])
            val_files.append(files[3])
            test_files.append(files[4])
        else:
            # ì˜ˆì™¸ ì²˜ë¦¬ (í˜¹ì‹œ ë‹¤ë¥¸ ê°œìˆ˜ê°€ ìžˆì„ ê²½ìš°)
            n_train = max(1, int(n * 0.6))
            n_val = max(1, int(n * 0.2))
            train_files.extend(files[:n_train])
            val_files.extend(files[n_train:n_train + n_val])
            test_files.extend(files[n_train + n_val:])

    print(f"Stratified ë¶„í•  ì™„ë£Œ:")
    print(f"  Train: {len(train_files)}ê°œ íŒŒì¼ ({len(train_files)*100/len(csv_files):.1f}%)")
    print(f"  Val:   {len(val_files)}ê°œ íŒŒì¼ ({len(val_files)*100/len(csv_files):.1f}%)")
    print(f"  Test:  {len(test_files)}ê°œ íŒŒì¼ ({len(test_files)*100/len(csv_files):.1f}%)")
    print(f"  â†’ ëª¨ë“  {len(routes)}ê°œ ê²½ë¡œê°€ Train/Val/Testì— í¬í•¨ë¨")
    print()

    # ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ì²˜ë¦¬
    n_workers = min(cpu_count(), 8)
    print(f"ë³‘ë ¬ ì²˜ë¦¬: {n_workers} workers\n")

    def process_split(files, split_name):
        print(f"ì²˜ë¦¬ ì¤‘: {split_name}")

        args_list = [(f, window_size, stride) for f in files]

        with Pool(n_workers) as pool:
            results = list(tqdm(
                pool.imap(process_file, args_list),
                total=len(files),
                desc=split_name
            ))

        # ìƒ˜í”Œ ìˆ˜ì§‘
        all_samples = []
        for samples in results:
            all_samples.extend(samples)

        # JSONL ì €ìž¥
        output_file = output_dir / f"{split_name}.jsonl"
        with output_file.open('w') as f:
            for sample in all_samples:
                f.write(json.dumps(sample) + '\n')

        print(f"  {split_name}: {len(all_samples)}ê°œ ìƒ˜í”Œ ì €ìž¥ â†’ {output_file}")
        return len(all_samples)

    # ê° split ì²˜ë¦¬
    n_train_samples = process_split(train_files, "train")
    n_val_samples = process_split(val_files, "val")
    n_test_samples = process_split(test_files, "test")

    # ë©”íƒ€ë°ì´í„° ì €ìž¥
    meta = {
        "n_features": 8,  # magx, magy, magz, magnitude, delta_magx, delta_magy, delta_magz, delta_magnitude
        "window_size": window_size,
        "stride": stride,
        "n_train": n_train_samples,
        "n_val": n_val_samples,
        "n_test": n_test_samples,
    }

    with (output_dir / "meta.json").open('w') as f:
        json.dump(meta, f, indent=2)

    print()
    print("=" * 80)
    print("âœ… ë³€í™˜ ì™„ë£Œ!")
    print(f"  ì¶œë ¥: {output_dir}")
    print(f"  Features: 8ê°œ (ê¸°ì¡´ 4ê°œ + Gradient 4ê°œ)")
    print(f"  Train: {n_train_samples:,}ê°œ ìƒ˜í”Œ")
    print(f"  Val:   {n_val_samples:,}ê°œ ìƒ˜í”Œ")
    print(f"  Test:  {n_test_samples:,}ê°œ ìƒ˜í”Œ")
    print("=" * 80)

if __name__ == "__main__":
    main()
