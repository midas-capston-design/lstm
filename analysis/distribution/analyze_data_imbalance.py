#!/usr/bin/env python3
"""ë°ì´í„° ë¶ˆê· í˜• ë¶„ì„: ê³µê°„ì  ë¶„í¬, ê²½ë¡œë³„ ìƒ˜í”Œ ìˆ˜, ì´ìƒì¹˜ ìœ„ì¹˜ íŒ¨í„´"""
import json
import sys
from pathlib import Path
import numpy as np
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'
    font_prop = fm.FontProperties(fname=font_path)
    plt.rcParams['font.family'] = font_prop.get_name()
    plt.rcParams['axes.unicode_minus'] = False
except:
    print("âš ï¸ í•œê¸€ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨")

# ì—­ì •ê·œí™”
COORD_CENTER = (-41.0, 0.0)
COORD_SCALE = 50.0

def denormalize_coord(x_norm: float, y_norm: float):
    x = x_norm * COORD_SCALE + COORD_CENTER[0]
    y = y_norm * COORD_SCALE + COORD_CENTER[1]
    return (x, y)

def load_raw_csv_info(raw_dir: Path):
    """ì›ë³¸ CSV íŒŒì¼ ì •ë³´ ë¡œë“œ"""
    csv_files = list(raw_dir.glob("*.csv"))

    path_to_files = defaultdict(list)
    for csv_file in csv_files:
        parts = csv_file.stem.split("_")
        if len(parts) >= 2:
            path_id = f"{parts[0]}_{parts[1]}"
            path_to_files[path_id].append(csv_file)

    return path_to_files

def analyze_data_imbalance(data_dir: Path, raw_dir: Path = None):
    """ë°ì´í„° ë¶ˆê· í˜• ë¶„ì„"""

    print("=" * 80)
    print("ğŸ” ë°ì´í„° ë¶ˆê· í˜• ë¶„ì„")
    print("=" * 80)
    print()

    # 1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
    splits = ['train', 'val', 'test']
    all_data = {}

    for split in splits:
        jsonl_path = data_dir / f"{split}.jsonl"
        samples = []
        with jsonl_path.open() as f:
            for line in f:
                samples.append(json.loads(line))
        all_data[split] = samples

    # 2. ì¢Œí‘œ ë¶„í¬ ë¶„ì„
    print("=" * 80)
    print("ğŸ“ ì¢Œí‘œ ê³µê°„ ë¶„í¬ ë¶„ì„")
    print("=" * 80)

    for split in splits:
        samples = all_data[split]
        targets = [s['target'] for s in samples]

        # ì—­ì •ê·œí™”
        coords = [denormalize_coord(t[0], t[1]) for t in targets]
        x_coords = [c[0] for c in coords]
        y_coords = [c[1] for c in coords]

        print(f"\n{split.upper()}:")
        print(f"  ìƒ˜í”Œ ìˆ˜: {len(samples)}")
        print(f"  X ë²”ìœ„: [{min(x_coords):.2f}, {max(x_coords):.2f}]m (mean={np.mean(x_coords):.2f})")
        print(f"  Y ë²”ìœ„: [{min(y_coords):.2f}, {max(y_coords):.2f}]m (mean={np.mean(y_coords):.2f})")

        # ê±´ë¬¼ ì¤‘ì‹¬ ê¸°ì¤€ ë¶„í¬ (COORD_CENTER = (-41, 0) ê¸°ì¤€)
        center_x, center_y = COORD_CENTER
        distances_from_center = [
            np.sqrt((x - center_x)**2 + (y - center_y)**2)
            for x, y in coords
        ]

        print(f"  ì¤‘ì‹¬ìœ¼ë¡œë¶€í„° ê±°ë¦¬:")
        print(f"    í‰ê· : {np.mean(distances_from_center):.2f}m")
        print(f"    ì¤‘ì•™ê°’: {np.median(distances_from_center):.2f}m")
        print(f"    í‘œì¤€í¸ì°¨: {np.std(distances_from_center):.2f}m")

        # ê±°ë¦¬ êµ¬ê°„ë³„ ìƒ˜í”Œ ìˆ˜
        bins = [0, 5, 10, 15, 20, 30, 100]
        labels = ['0-5m', '5-10m', '10-15m', '15-20m', '20-30m', '>30m']

        print(f"  ê±°ë¦¬ êµ¬ê°„ë³„ ë¶„í¬:")
        for i in range(len(bins)-1):
            count = sum(bins[i] <= d < bins[i+1] for d in distances_from_center)
            pct = count / len(distances_from_center) * 100
            print(f"    {labels[i]:<10}: {count:>5}ê°œ ({pct:>5.1f}%)")

    print()

    # 3. ì›ë³¸ CSV ê¸°ë°˜ ê²½ë¡œë³„ ìƒ˜í”Œ ìˆ˜ ë¶„ì„
    if raw_dir and raw_dir.exists():
        print("=" * 80)
        print("ğŸ“Š ì›ë³¸ ë°ì´í„° ê²½ë¡œë³„ íŒŒì¼ ìˆ˜")
        print("=" * 80)

        path_to_files = load_raw_csv_info(raw_dir)

        # ê²½ë¡œë³„ íŒŒì¼ ìˆ˜ í†µê³„
        file_counts = [len(files) for files in path_to_files.values()]

        print(f"\nì´ ê²½ë¡œ ìˆ˜: {len(path_to_files)}")
        print(f"ê²½ë¡œë‹¹ íŒŒì¼ ìˆ˜:")
        print(f"  í‰ê· : {np.mean(file_counts):.2f}ê°œ")
        print(f"  ì¤‘ì•™ê°’: {np.median(file_counts):.0f}ê°œ")
        print(f"  ìµœì†Œ: {min(file_counts)}ê°œ")
        print(f"  ìµœëŒ€: {max(file_counts)}ê°œ")
        print(f"  í‘œì¤€í¸ì°¨: {np.std(file_counts):.2f}ê°œ")

        # íŒŒì¼ ìˆ˜ ë¶„í¬
        print(f"\níŒŒì¼ ìˆ˜ ë¶„í¬:")
        file_count_dist = Counter(file_counts)
        for count in sorted(file_count_dist.keys()):
            num_paths = file_count_dist[count]
            print(f"  {count}ê°œ íŒŒì¼: {num_paths}ê°œ ê²½ë¡œ")

        # ìƒìœ„/í•˜ìœ„ 10ê°œ ê²½ë¡œ
        sorted_paths = sorted(path_to_files.items(), key=lambda x: len(x[1]), reverse=True)

        print(f"\nğŸ“ˆ íŒŒì¼ ìˆ˜ ìƒìœ„ 10ê°œ ê²½ë¡œ:")
        for path_id, files in sorted_paths[:10]:
            print(f"  {path_id}: {len(files)}ê°œ")

        print(f"\nğŸ“‰ íŒŒì¼ ìˆ˜ í•˜ìœ„ 10ê°œ ê²½ë¡œ:")
        for path_id, files in sorted_paths[-10:]:
            print(f"  {path_id}: {len(files)}ê°œ")

        print()

    # 4. ì „ì²˜ë¦¬ í›„ ê²½ë¡œë³„ ìƒ˜í”Œ ìˆ˜ ì¶”ì •
    print("=" * 80)
    print("ğŸ”¢ ì „ì²˜ë¦¬ í›„ ìƒ˜í”Œ ìˆ˜ ë¶„ì„ (window=250, stride=50)")
    print("=" * 80)

    # meta.jsonì—ì„œ window size, stride í™•ì¸
    meta_path = data_dir / "meta.json"
    with meta_path.open() as f:
        meta = json.load(f)

    window_size = meta['window_size']
    stride = meta['stride']

    print(f"\nì „ì²˜ë¦¬ ì„¤ì •:")
    print(f"  Window size: {window_size}")
    print(f"  Stride: {stride}")
    print()

    # CSV íŒŒì¼ ê¸¸ì´ë³„ ìƒ˜í”Œ ìƒì„± ê°œìˆ˜ ê³„ì‚°
    if raw_dir and raw_dir.exists():
        import csv

        csv_files = list(raw_dir.glob("*.csv"))[:50]  # ìƒ˜í”Œë§ (50ê°œë§Œ)
        csv_lengths = []

        for csv_file in csv_files:
            with csv_file.open() as f:
                reader = csv.DictReader(f)
                length = sum(1 for _ in reader)
                csv_lengths.append(length)

        csv_lengths = np.array(csv_lengths)

        # ì˜ˆìƒ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
        expected_samples = []
        for length in csv_lengths:
            if length >= window_size:
                n_samples = (length - window_size) // stride + 1
                expected_samples.append(n_samples)

        print(f"CSV íŒŒì¼ ë¶„ì„ (ìƒ˜í”Œ {len(csv_files)}ê°œ):")
        print(f"  CSV í‰ê·  ê¸¸ì´: {np.mean(csv_lengths):.0f} ìŠ¤í…")
        print(f"  CSVë‹¹ í‰ê·  ìƒ˜í”Œ ìƒì„±: {np.mean(expected_samples):.1f}ê°œ")
        print(f"  ìƒ˜í”Œ ìƒì„± ë²”ìœ„: [{min(expected_samples)}, {max(expected_samples)}]ê°œ")
        print()

        # ê¸¸ì´ ë¶„í¬
        print(f"CSV ê¸¸ì´ ë¶„í¬:")
        length_bins = [0, 300, 400, 500, 600, 1000, 10000]
        length_labels = ['<300', '300-400', '400-500', '500-600', '600-1000', '>1000']

        for i in range(len(length_bins)-1):
            count = sum(length_bins[i] <= l < length_bins[i+1] for l in csv_lengths)
            pct = count / len(csv_lengths) * 100
            print(f"  {length_labels[i]:<12}: {count:>3}ê°œ ({pct:>5.1f}%)")
        print()

    # 5. ì‹œê°í™”
    output_dir = Path(__file__).parent / "outputs"
    output_dir.mkdir(exist_ok=True)

    fig = plt.figure(figsize=(18, 12))

    # 5-1. ì¢Œí‘œ ê³µê°„ ë¶„í¬ (ê° splitë³„)
    for idx, split in enumerate(splits):
        samples = all_data[split]
        targets = [s['target'] for s in samples]
        coords = [denormalize_coord(t[0], t[1]) for t in targets]
        x_coords = [c[0] for c in coords]
        y_coords = [c[1] for c in coords]

        ax = plt.subplot(2, 3, idx + 1)

        # 2D íˆìŠ¤í† ê·¸ë¨
        h = ax.hist2d(x_coords, y_coords, bins=30, cmap='YlOrRd')
        ax.scatter(*COORD_CENTER, c='blue', s=200, marker='X',
                  edgecolors='black', linewidths=2, label='ì¤‘ì‹¬', zorder=5)
        ax.set_xlabel('X (m)', fontproperties=font_prop)
        ax.set_ylabel('Y (m)', fontproperties=font_prop)
        ax.set_title(f'{split.upper()} ì¢Œí‘œ ë¶„í¬ ({len(samples)}ê°œ)', fontproperties=font_prop)
        ax.legend(prop=font_prop)
        ax.grid(True, alpha=0.3)
        ax.set_aspect('equal')
        plt.colorbar(h[3], ax=ax, label='ìƒ˜í”Œ ìˆ˜')

    # 5-2. ì¤‘ì‹¬ìœ¼ë¡œë¶€í„° ê±°ë¦¬ ë¶„í¬
    ax = plt.subplot(2, 3, 4)

    for split in splits:
        samples = all_data[split]
        targets = [s['target'] for s in samples]
        coords = [denormalize_coord(t[0], t[1]) for t in targets]

        center_x, center_y = COORD_CENTER
        distances = [
            np.sqrt((x - center_x)**2 + (y - center_y)**2)
            for x, y in coords
        ]

        ax.hist(distances, bins=30, alpha=0.5, label=split.upper(), edgecolor='black')

    ax.set_xlabel('ì¤‘ì‹¬ìœ¼ë¡œë¶€í„° ê±°ë¦¬ (m)', fontproperties=font_prop)
    ax.set_ylabel('ìƒ˜í”Œ ìˆ˜', fontproperties=font_prop)
    ax.set_title('ì¤‘ì‹¬ìœ¼ë¡œë¶€í„° ê±°ë¦¬ ë¶„í¬', fontproperties=font_prop)
    ax.legend(prop=font_prop)
    ax.grid(True, alpha=0.3)

    # 5-3. ê²½ë¡œë³„ íŒŒì¼ ìˆ˜ ë¶„í¬ (ìˆìœ¼ë©´)
    if raw_dir and raw_dir.exists():
        ax = plt.subplot(2, 3, 5)

        path_to_files = load_raw_csv_info(raw_dir)
        file_counts = [len(files) for files in path_to_files.values()]

        ax.hist(file_counts, bins=20, edgecolor='black', alpha=0.7)
        ax.axvline(np.mean(file_counts), color='red', linestyle='--',
                  label=f'í‰ê· : {np.mean(file_counts):.1f}')
        ax.axvline(np.median(file_counts), color='blue', linestyle='--',
                  label=f'ì¤‘ì•™ê°’: {np.median(file_counts):.0f}')
        ax.set_xlabel('ê²½ë¡œë‹¹ CSV íŒŒì¼ ìˆ˜', fontproperties=font_prop)
        ax.set_ylabel('ê²½ë¡œ ìˆ˜', fontproperties=font_prop)
        ax.set_title('ê²½ë¡œë³„ ì›ë³¸ íŒŒì¼ ìˆ˜ ë¶„í¬', fontproperties=font_prop)
        ax.legend(prop=font_prop)
        ax.grid(True, alpha=0.3)

    # 5-4. Splitë³„ ìƒ˜í”Œ ìˆ˜
    ax = plt.subplot(2, 3, 6)

    split_counts = [len(all_data[s]) for s in splits]
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    bars = ax.bar(splits, split_counts, color=colors, edgecolor='black', alpha=0.7)

    # ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ
    for bar, count in zip(bars, split_counts):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{count}',
               ha='center', va='bottom', fontproperties=font_prop, fontsize=12)

    ax.set_ylabel('ìƒ˜í”Œ ìˆ˜', fontproperties=font_prop)
    ax.set_title('Splitë³„ ìƒ˜í”Œ ìˆ˜', fontproperties=font_prop)
    ax.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    output_path = output_dir / "data_imbalance_analysis.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š ì‹œê°í™” ì €ì¥: {output_path}")
    print()

    # 6. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
    print("=" * 80)
    print("ğŸ“‹ ë¶„ì„ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­")
    print("=" * 80)

    # ë°ì´í„° ë¶ˆê· í˜• ì—¬ë¶€ íŒë‹¨
    all_samples = []
    for split in splits:
        samples = all_data[split]
        targets = [s['target'] for s in samples]
        coords = [denormalize_coord(t[0], t[1]) for t in targets]
        all_samples.extend(coords)

    center_x, center_y = COORD_CENTER
    all_distances = [
        np.sqrt((x - center_x)**2 + (y - center_y)**2)
        for x, y in all_samples
    ]

    # ì¤‘ì‹¬ ì§‘ì¤‘ë„ ì¸¡ì • (ì¤‘ì‹¬ 10m ì´ë‚´ ë¹„ìœ¨)
    central_ratio = sum(d <= 10 for d in all_distances) / len(all_distances)

    print()
    if central_ratio > 0.3:
        print(f"âš ï¸ ì¤‘ì‹¬ ì§‘ì¤‘ë„ ë†’ìŒ: {central_ratio*100:.1f}%ê°€ ì¤‘ì‹¬ 10m ì´ë‚´")
        print("  â†’ ê±´ë¬¼ ì¤‘ì‹¬ ë°ì´í„° ê³¼ë‹¤")
        print("  â†’ ì™¸ê³½ ì˜ì—­ ì˜ˆì¸¡ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥")
        print()
        print("ğŸ’¡ ê¶Œì¥ ì‚¬í•­:")
        print("  1. ê²½ë¡œë³„ ìƒ˜í”Œë§ (ê° ê²½ë¡œì—ì„œ ë™ì¼í•œ ìƒ˜í”Œ ìˆ˜ ì¶”ì¶œ)")
        print("  2. ê³µê°„ë³„ ê°€ì¤‘ì¹˜ (ì™¸ê³½ ì˜ì—­ ìƒ˜í”Œì— ë†’ì€ ê°€ì¤‘ì¹˜)")
        print("  3. ë°ì´í„° ì¦ê°• (ì™¸ê³½ ê²½ë¡œ ë°ì´í„° augmentation)")
    else:
        print(f"âœ… ë°ì´í„° ë¶„í¬ ì–‘í˜¸: ì¤‘ì‹¬ ì§‘ì¤‘ë„ {central_ratio*100:.1f}%")

    print()

    # ê²½ë¡œë³„ ë¶ˆê· í˜•
    if raw_dir and raw_dir.exists():
        path_to_files = load_raw_csv_info(raw_dir)
        file_counts = [len(files) for files in path_to_files.values()]

        max_count = max(file_counts)
        min_count = min(file_counts)
        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')

        if imbalance_ratio > 3:
            print(f"âš ï¸ ê²½ë¡œë³„ íŒŒì¼ ìˆ˜ ë¶ˆê· í˜•: {imbalance_ratio:.1f}x ì°¨ì´")
            print(f"  ìµœëŒ€: {max_count}ê°œ, ìµœì†Œ: {min_count}ê°œ")
            print()
            print("ğŸ’¡ ê¶Œì¥ ì‚¬í•­:")
            print("  1. ê²½ë¡œë³„ ê· ë“± ìƒ˜í”Œë§ (--split-modeë¥¼ 'balanced'ë¡œ)")
            print("  2. ì ì€ ê²½ë¡œëŠ” augmentationìœ¼ë¡œ ë³´ê°•")
        else:
            print(f"âœ… ê²½ë¡œë³„ ê· í˜• ì–‘í˜¸: ìµœëŒ€/ìµœì†Œ ë¹„ìœ¨ {imbalance_ratio:.1f}x")

    print()
    print("=" * 80)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Analyze data imbalance")
    parser.add_argument("--data-dir", type=str, default="data/sliding_mag4")
    parser.add_argument("--raw-dir", type=str, default="data/raw",
                       help="ì›ë³¸ CSV ë””ë ‰í† ë¦¬ (ê²½ë¡œë³„ ë¶„ì„ìš©)")

    args = parser.parse_args()

    raw_dir = Path(args.raw_dir) if args.raw_dir else None

    analyze_data_imbalance(
        data_dir=Path(args.data_dir),
        raw_dir=raw_dir
    )
