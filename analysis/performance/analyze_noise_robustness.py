#!/usr/bin/env python3
"""ë…¸ì´ì¦ˆ ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ ë¶„ì„ - ì™œ Ïƒ=0.5ì—ì„œ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ë˜ëŠ”ê°€?"""
import json
import sys
from pathlib import Path
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'
    font_prop = fm.FontProperties(fname=font_path)
    plt.rcParams['font.family'] = font_prop.get_name()
    plt.rcParams['axes.unicode_minus'] = False
except:
    print("âš ï¸ í•œê¸€ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨")

sys.path.append(str(Path(__file__).parent.parent / "src"))
from model import HyenaPositioning
from torch.utils.data import Dataset, DataLoader

# ì—­ì •ê·œí™”
COORD_CENTER = (-41.0, 0.0)
COORD_SCALE = 50.0

def denormalize_coord(x_norm: float, y_norm: float):
    x = x_norm * COORD_SCALE + COORD_CENTER[0]
    y = y_norm * COORD_SCALE + COORD_CENTER[1]
    return (x, y)

class SlidingWindowDataset(Dataset):
    def __init__(self, jsonl_path: Path):
        self.samples = []
        with jsonl_path.open() as f:
            for line in f:
                self.samples.append(json.loads(line))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        features = torch.tensor(sample["features"], dtype=torch.float32)
        target = torch.tensor(sample["target"], dtype=torch.float32)
        return features, target

def analyze_noise_impact(
    checkpoint_path: Path,
    data_dir: Path,
    device: str = "cpu",
):
    """ë…¸ì´ì¦ˆ ì˜í–¥ ë¶„ì„"""

    print("=" * 80)
    print("ğŸ” ë…¸ì´ì¦ˆ ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ ìƒì„¸ ë¶„ì„")
    print("=" * 80)
    print(f"  Checkpoint: {checkpoint_path}")
    print(f"  Data dir: {data_dir}")
    print()

    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    meta_path = data_dir / "meta.json"
    with meta_path.open() as f:
        meta = json.load(f)
    n_features = meta["n_features"]

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_path = data_dir / "test.jsonl"
    test_ds = SlidingWindowDataset(test_path)
    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)

    print(f"ğŸ“Š Test samples: {len(test_ds)}ê°œ")
    print()

    # Device ì„¤ì •
    if device == "cuda" and torch.cuda.is_available():
        device = torch.device("cuda")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ Apple Silicon GPU (MPS) ì‚¬ìš©")
    else:
        device = torch.device("cpu")
        print("ğŸ’» CPU ì‚¬ìš©")

    # ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ”„ Checkpoint ë¡œë“œ ì¤‘...")
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    model = HyenaPositioning(
        input_dim=n_features,
        hidden_dim=384,
        output_dim=2,
        depth=10,
        dropout=0.1,
        num_edge_types=1,
    ).to(device)

    model.load_state_dict(checkpoint["model_state"])
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print()

    # 1. ë°ì´í„° í†µê³„ ë¶„ì„
    print("=" * 80)
    print("ğŸ“Š ì…ë ¥ ë°ì´í„° í†µê³„ ë¶„ì„")
    print("=" * 80)

    all_features = []
    for features, _ in test_loader:
        all_features.append(features)
    all_features = torch.cat(all_features, dim=0)

    # Featureë³„ í†µê³„ (MagX, MagY, MagZ, Magnitude)
    feature_names = ["MagX", "MagY", "MagZ", "Magnitude"]
    print(f"{'Feature':<12} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10}")
    print("-" * 80)

    for i, name in enumerate(feature_names):
        feat = all_features[:, :, i]
        print(f"{name:<12} {feat.mean():.6f}  {feat.std():.6f}  {feat.min():.6f}  {feat.max():.6f}")

    print()
    print("âš ï¸ ë¬¸ì œì  ì§„ë‹¨:")
    print("  í˜„ì¬ ë…¸ì´ì¦ˆ ì¶”ê°€ ë°©ì‹: noise = randn_like(features) * Ïƒ")
    print("  â†’ ì •ê·œí™”ëœ ë°ì´í„° (meanâ‰ˆ0, stdâ‰ˆ1)ì— ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€")
    print()
    print("  Ïƒ=0.5ì¼ ë•Œ:")
    print("  - ì›ë³¸ ì‹ í˜¸ ëŒ€ë¹„ ë…¸ì´ì¦ˆ ë¹„ìœ¨ (SNR) ë§¤ìš° ë‚®ìŒ")
    print("  - ì •ê·œí™”ëœ ê°’ì˜ 50%ì— í•´ë‹¹í•˜ëŠ” ë…¸ì´ì¦ˆ ì¶”ê°€")
    print("  - ì‹¤ì œ ì„¼ì„œ ë…¸ì´ì¦ˆë³´ë‹¤ í›¨ì”¬ í° ê°’!")
    print()

    # 2. ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ë ˆë²¨ì—ì„œ ì„±ëŠ¥ ì¸¡ì •
    print("=" * 80)
    print("ğŸ“ˆ ë…¸ì´ì¦ˆ ë ˆë²¨ë³„ ì„±ëŠ¥ ë¶„ì„")
    print("=" * 80)

    noise_levels = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0]
    results = {}

    for noise_std in noise_levels:
        errors = []

        with torch.no_grad():
            for features, targets in test_loader:
                features = features.to(device)
                targets = targets.to(device)

                # ë…¸ì´ì¦ˆ ì¶”ê°€
                if noise_std > 0:
                    noise = torch.randn_like(features) * noise_std
                    noisy_features = features + noise
                else:
                    noisy_features = features

                edge_ids = torch.zeros(features.size(0), dtype=torch.long, device=device)
                outputs = model(noisy_features, edge_ids)
                pred = outputs[:, -1, :]

                pred_np = pred.cpu().numpy()
                target_np = targets.cpu().numpy()

                for i in range(len(pred_np)):
                    pred_pos = denormalize_coord(pred_np[i, 0], pred_np[i, 1])
                    target_pos = denormalize_coord(target_np[i, 0], target_np[i, 1])
                    dist = abs(pred_pos[0] - target_pos[0]) + abs(pred_pos[1] - target_pos[1])
                    errors.append(dist)

        errors = np.array(errors)
        mae = np.mean(errors)
        median = np.median(errors)
        p90 = np.percentile(errors, 90)

        results[noise_std] = {
            'mae': mae,
            'median': median,
            'p90': p90,
            'errors': errors
        }

        baseline_mae = results[0.0]['mae']
        degradation = ((mae - baseline_mae) / baseline_mae * 100) if noise_std > 0 else 0

        print(f"Ïƒ={noise_std:>4.2f}: MAE={mae:>6.3f}m, P90={p90:>6.3f}m, "
              f"Degradation={degradation:>6.1f}%")

    print()

    # 3. ì‹œê°í™”
    output_dir = Path(__file__).parent / "outputs"
    output_dir.mkdir(exist_ok=True)

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # (1) ë…¸ì´ì¦ˆ ë ˆë²¨ vs MAE
    ax = axes[0, 0]
    noise_vals = list(results.keys())
    mae_vals = [results[n]['mae'] for n in noise_vals]
    ax.plot(noise_vals, mae_vals, 'o-', linewidth=2, markersize=8)
    ax.axhline(results[0.0]['mae'], color='green', linestyle='--', label='Baseline (no noise)')
    ax.axvline(0.1, color='orange', linestyle='--', alpha=0.5, label='Ïƒ=0.1')
    ax.axvline(0.5, color='red', linestyle='--', alpha=0.5, label='Ïƒ=0.5')
    ax.set_xlabel('Noise Ïƒ', fontproperties=font_prop)
    ax.set_ylabel('MAE (m)', fontproperties=font_prop)
    ax.set_title('ë…¸ì´ì¦ˆ ë ˆë²¨ vs MAE', fontproperties=font_prop)
    ax.grid(True, alpha=0.3)
    ax.legend(prop=font_prop)

    # (2) ë…¸ì´ì¦ˆ ë ˆë²¨ vs P90
    ax = axes[0, 1]
    p90_vals = [results[n]['p90'] for n in noise_vals]
    ax.plot(noise_vals, p90_vals, 'o-', linewidth=2, markersize=8, color='purple')
    ax.axhline(results[0.0]['p90'], color='green', linestyle='--', label='Baseline P90')
    ax.axvline(0.1, color='orange', linestyle='--', alpha=0.5)
    ax.axvline(0.5, color='red', linestyle='--', alpha=0.5)
    ax.set_xlabel('Noise Ïƒ', fontproperties=font_prop)
    ax.set_ylabel('P90 (m)', fontproperties=font_prop)
    ax.set_title('ë…¸ì´ì¦ˆ ë ˆë²¨ vs P90', fontproperties=font_prop)
    ax.grid(True, alpha=0.3)
    ax.legend(prop=font_prop)

    # (3) SNR ë¶„ì„
    ax = axes[1, 0]
    # SNR = 20*log10(signal_std / noise_std)
    signal_std = all_features.std().item()
    snr_vals = [20 * np.log10(signal_std / n) if n > 0 else 100 for n in noise_vals[1:]]
    ax.plot(noise_vals[1:], snr_vals, 's-', linewidth=2, markersize=8, color='red')
    ax.set_xlabel('Noise Ïƒ', fontproperties=font_prop)
    ax.set_ylabel('SNR (dB)', fontproperties=font_prop)
    ax.set_title('ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„ (SNR)', fontproperties=font_prop)
    ax.grid(True, alpha=0.3)
    ax.axhline(0, color='black', linestyle='-', linewidth=0.5)

    # (4) ì˜¤ì°¨ ë¶„í¬ ë¹„êµ (Ïƒ=0, 0.1, 0.5)
    ax = axes[1, 1]
    for noise_std in [0.0, 0.1, 0.5]:
        errors = results[noise_std]['errors']
        sorted_errors = np.sort(errors)
        cdf = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100
        label = f'Ïƒ={noise_std}' if noise_std > 0 else 'No noise'
        ax.plot(sorted_errors, cdf, linewidth=2, label=label)
    ax.set_xlabel('Error (m)', fontproperties=font_prop)
    ax.set_ylabel('Cumulative %', fontproperties=font_prop)
    ax.set_title('ì˜¤ì°¨ ë¶„í¬ ë¹„êµ (CDF)', fontproperties=font_prop)
    ax.legend(prop=font_prop)
    ax.grid(True, alpha=0.3)
    ax.set_xlim(0, 15)

    plt.tight_layout()
    output_path = output_dir / "noise_robustness_analysis.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š ì‹œê°í™” ì €ì¥: {output_path}")
    print()

    # 4. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
    print("=" * 80)
    print("ğŸ’¡ ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­")
    print("=" * 80)
    print()
    print("ğŸ” ë¬¸ì œì˜ ì›ì¸:")
    print("  1. í˜„ì¬ ë…¸ì´ì¦ˆ ì¶”ê°€ ë°©ì‹ì´ ë¹„í˜„ì‹¤ì ìœ¼ë¡œ í¼")
    print(f"     - ë°ì´í„° í‘œì¤€í¸ì°¨: {signal_std:.6f}")
    print(f"     - Ïƒ=0.5 ë…¸ì´ì¦ˆëŠ” ì‹ í˜¸ì˜ 50% ìˆ˜ì¤€!")
    print()
    print("  2. ì‹¤ì œ ì„¼ì„œ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ê³¼ ê´´ë¦¬")
    print("     - ì‹¤ì œ ì§€ìê¸° ì„¼ì„œ ë…¸ì´ì¦ˆ: ~0.01-0.02 ìˆ˜ì¤€ (ì •ê·œí™” í›„)")
    print("     - í˜„ì¬ í…ŒìŠ¤íŠ¸: Ïƒ=0.5ëŠ” ì‹¤ì œë³´ë‹¤ 25-50ë°° í¼")
    print()
    print("  3. SNR ê´€ì ì—ì„œ ë³´ë©´:")
    signal_std = all_features.std().item()
    snr_01 = 20 * np.log10(signal_std / 0.1)
    snr_05 = 20 * np.log10(signal_std / 0.5)
    print(f"     - Ïƒ=0.1: SNR={snr_01:.1f}dB (ì–‘í˜¸)")
    print(f"     - Ïƒ=0.5: SNR={snr_05:.1f}dB (ë§¤ìš° ë‚˜ì¨)")
    print()
    print("âœ… ê¶Œì¥ì‚¬í•­:")
    print("  1. ì ì ˆí•œ ë…¸ì´ì¦ˆ ë ˆë²¨ ì‚¬ìš©:")
    print("     - ì‹¤ì œ ì„¼ì„œ ë…¸ì´ì¦ˆ: Ïƒ=0.01~0.02")
    print("     - ê·¹í•œ í…ŒìŠ¤íŠ¸: Ïƒ=0.05~0.1")
    print("     - Ïƒ=0.5ëŠ” ë¹„í˜„ì‹¤ì ")
    print()
    print("  2. ëŒ€ì•ˆì  ë…¸ì´ì¦ˆ ì¶”ê°€ ë°©ë²•:")
    print("     - Featureë³„ ë‹¤ë¥¸ ë…¸ì´ì¦ˆ ë ˆë²¨ (MagX/Y/ZëŠ” í¬ê²Œ, MagnitudeëŠ” ì‘ê²Œ)")
    print("     - ì‹œê°„ì¶• ìƒê´€ì„± ìˆëŠ” ë…¸ì´ì¦ˆ (ì‹¤ì œ ì„¼ì„œ drift ëª¨ì‚¬)")
    print("     - Dropout ë…¸ì´ì¦ˆ (ì¼ë¶€ íƒ€ì„ìŠ¤í… ëœë¤ ì œê±°)")
    print()
    print("  3. ë…¸ì´ì¦ˆ ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ ê°œì„ :")
    print("     - í•™ìŠµ ì‹œ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì†ŒëŸ‰ ë…¸ì´ì¦ˆ ì¶”ê°€ (Ïƒ=0.01~0.02)")
    print("     - Wavelet denoising ê°•í™”")
    print()
    print("=" * 80)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Analyze noise robustness")
    parser.add_argument("--checkpoint", type=str, default="models/hyena_mag4/checkpoints/best.pt")
    parser.add_argument("--data-dir", type=str, default="data/sliding_mag4")
    parser.add_argument("--cpu", action="store_true", help="Force CPU usage")

    args = parser.parse_args()

    device = "cpu" if args.cpu else ("cuda" if torch.cuda.is_available() else "cpu")

    analyze_noise_impact(
        checkpoint_path=Path(args.checkpoint),
        data_dir=Path(args.data_dir),
        device=device,
    )
