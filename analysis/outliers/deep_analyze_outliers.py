#!/usr/bin/env python3
"""ì‹¬ì¸µ ì•„ì›ƒë¼ì´ì–´ ë¶„ì„: Train/Val/Test ëª¨ë‘ ë¶„ì„ + ì „ì²˜ë¦¬ ì¼ê´€ì„± ê²€ì¦"""
import json
import sys
from pathlib import Path
import torch
import torch.nn as nn
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'
    font_prop = fm.FontProperties(fname=font_path)
    plt.rcParams['font.family'] = font_prop.get_name()
    plt.rcParams['axes.unicode_minus'] = False
except:
    print("âš ï¸ í•œê¸€ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent / "src"))
from model import HyenaPositioning
from torch.utils.data import Dataset, DataLoader

# ì—­ì •ê·œí™”
COORD_CENTER = (-41.0, 0.0)
COORD_SCALE = 50.0

def denormalize_coord(x_norm: float, y_norm: float):
    x = x_norm * COORD_SCALE + COORD_CENTER[0]
    y = y_norm * COORD_SCALE + COORD_CENTER[1]
    return (x, y)

class SlidingWindowDataset(Dataset):
    def __init__(self, jsonl_path: Path):
        self.samples = []
        with jsonl_path.open() as f:
            for line in f:
                self.samples.append(json.loads(line))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        features = torch.tensor(sample["features"], dtype=torch.float32)
        target = torch.tensor(sample["target"], dtype=torch.float32)
        return features, target, idx  # idx ì¶”ê°€

def analyze_split(model, loader, split_name, device, dataset):
    """í•œ splitì˜ ì˜ˆì¸¡ ë° ì˜¤ì°¨ ë¶„ì„"""
    all_errors = []
    all_predictions = []
    all_targets = []
    all_indices = []
    all_features = []

    model.eval()
    with torch.no_grad():
        for features, targets, indices in loader:
            features = features.to(device)
            targets = targets.to(device)

            edge_ids = torch.zeros(features.size(0), dtype=torch.long, device=device)
            outputs = model(features, edge_ids)
            pred = outputs[:, -1, :]

            pred_np = pred.cpu().numpy()
            target_np = targets.cpu().numpy()
            features_np = features.cpu().numpy()

            for i in range(len(pred_np)):
                pred_pos = denormalize_coord(pred_np[i, 0], pred_np[i, 1])
                target_pos = denormalize_coord(target_np[i, 0], target_np[i, 1])

                # Manhattan distance
                dist = abs(pred_pos[0] - target_pos[0]) + abs(pred_pos[1] - target_pos[1])

                all_errors.append(dist)
                all_predictions.append(pred_pos)
                all_targets.append(target_pos)
                all_indices.append(indices[i].item())
                all_features.append(features_np[i])

    return {
        'errors': np.array(all_errors),
        'predictions': all_predictions,
        'targets': all_targets,
        'indices': all_indices,
        'features': all_features,
        'split': split_name
    }

def check_preprocessing_consistency(data_dir: Path):
    """ì „ì²˜ë¦¬ ì¼ê´€ì„± ê²€ì¦"""
    print("=" * 80)
    print("ğŸ” ì „ì²˜ë¦¬ ì¼ê´€ì„± ê²€ì¦")
    print("=" * 80)

    splits = ['train', 'val', 'test']
    stats = {}

    for split in splits:
        jsonl_path = data_dir / f"{split}.jsonl"
        samples = []
        with jsonl_path.open() as f:
            for line in f:
                samples.append(json.loads(line))

        # Feature í†µê³„
        all_features = []
        all_targets = []
        window_sizes = []

        for sample in samples:
            features = np.array(sample['features'])
            target = np.array(sample['target'])

            all_features.append(features)
            all_targets.append(target)
            window_sizes.append(len(features))

        all_features = np.array(all_features)  # [N, 250, n_features]
        all_targets = np.array(all_targets)    # [N, 2]

        # í†µê³„ ê³„ì‚°
        stats[split] = {
            'n_samples': len(samples),
            'window_sizes': window_sizes,
            'feature_mean': np.mean(all_features),
            'feature_std': np.std(all_features),
            'feature_min': np.min(all_features),
            'feature_max': np.max(all_features),
            'target_mean': np.mean(all_targets, axis=0),
            'target_std': np.std(all_targets, axis=0),
            'target_min': np.min(all_targets, axis=0),
            'target_max': np.max(all_targets, axis=0),
        }

    # ì¶œë ¥
    print(f"\n{'Split':<10} {'Samples':<10} {'Feature Mean':<15} {'Feature Std':<15} {'Target Mean':<25}")
    print("-" * 80)
    for split in splits:
        s = stats[split]
        print(f"{split:<10} {s['n_samples']:<10} {s['feature_mean']:<15.6f} {s['feature_std']:<15.6f} ({s['target_mean'][0]:>6.3f}, {s['target_mean'][1]:>6.3f})")

    print()
    print("ğŸ“Š ì„¸ë¶€ í†µê³„:")
    print("-" * 80)
    for split in splits:
        s = stats[split]
        print(f"\n{split.upper()}:")
        print(f"  Window sizes: {set(s['window_sizes'])} (ëª¨ë‘ ë™ì¼í•´ì•¼ í•¨)")
        print(f"  Feature range: [{s['feature_min']:.3f}, {s['feature_max']:.3f}]")
        print(f"  Target range X: [{s['target_min'][0]:.3f}, {s['target_max'][0]:.3f}]")
        print(f"  Target range Y: [{s['target_min'][1]:.3f}, {s['target_max'][1]:.3f}]")

    # ì¼ê´€ì„± ì²´í¬
    print()
    print("=" * 80)
    print("âœ… ì¼ê´€ì„± ì²´í¬")
    print("=" * 80)

    # Window size ì¼ê´€ì„±
    all_window_sizes = [set(stats[s]['window_sizes']) for s in splits]
    if len(set.union(*all_window_sizes)) == 1:
        print("âœ… Window size ì¼ê´€ì„±: OK (ëª¨ë‘ ë™ì¼)")
    else:
        print("âŒ Window size ì¼ê´€ì„±: FAIL (ë‹¤ë¦„)")
        for split in splits:
            print(f"  {split}: {set(stats[split]['window_sizes'])}")

    # Feature ë¶„í¬ ìœ ì‚¬ì„±
    feature_means = [stats[s]['feature_mean'] for s in splits]
    feature_stds = [stats[s]['feature_std'] for s in splits]

    mean_diff = max(feature_means) - min(feature_means)
    std_diff = max(feature_stds) - min(feature_stds)

    if mean_diff < 0.1 and std_diff < 0.1:
        print(f"âœ… Feature ì •ê·œí™” ì¼ê´€ì„±: OK (mean diff={mean_diff:.6f}, std diff={std_diff:.6f})")
    else:
        print(f"âš ï¸ Feature ì •ê·œí™” ì°¨ì´ ìˆìŒ: mean diff={mean_diff:.6f}, std diff={std_diff:.6f}")

    # Target ë¶„í¬ ìœ ì‚¬ì„±
    target_means_x = [stats[s]['target_mean'][0] for s in splits]
    target_means_y = [stats[s]['target_mean'][1] for s in splits]

    mean_diff_x = max(target_means_x) - min(target_means_x)
    mean_diff_y = max(target_means_y) - min(target_means_y)

    if mean_diff_x < 0.1 and mean_diff_y < 0.1:
        print(f"âœ… Target ë¶„í¬ ì¼ê´€ì„±: OK (X diff={mean_diff_x:.6f}, Y diff={mean_diff_y:.6f})")
    else:
        print(f"âš ï¸ Target ë¶„í¬ ì°¨ì´ ìˆìŒ: X diff={mean_diff_x:.6f}, Y diff={mean_diff_y:.6f}")

    print()
    return stats

def deep_analyze_outliers(
    checkpoint_path: Path,
    data_dir: Path,
    threshold: float = 5.0,
    device: str = "cpu",
):
    """ì‹¬ì¸µ ì•„ì›ƒë¼ì´ì–´ ë¶„ì„: Train/Val/Test ì „ë¶€"""

    print("=" * 80)
    print("ğŸ”¬ ì‹¬ì¸µ ì•„ì›ƒë¼ì´ì–´ ë¶„ì„ (Train/Val/Test)")
    print("=" * 80)
    print(f"  Checkpoint: {checkpoint_path}")
    print(f"  Data dir: {data_dir}")
    print(f"  Outlier threshold: {threshold}m")
    print()

    # 1. ì „ì²˜ë¦¬ ì¼ê´€ì„± ê²€ì¦
    preprocessing_stats = check_preprocessing_consistency(data_dir)

    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    meta_path = data_dir / "meta.json"
    with meta_path.open() as f:
        meta = json.load(f)

    n_features = meta["n_features"]

    # Device ì„¤ì •
    if device == "cuda" and torch.cuda.is_available():
        device = torch.device("cuda")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ Apple Silicon GPU (MPS) ì‚¬ìš©")
    else:
        device = torch.device("cpu")
        print("ğŸ’» CPU ì‚¬ìš©")

    # ëª¨ë¸ ë¡œë“œ
    print()
    print("ğŸ”„ Checkpoint ë¡œë“œ ì¤‘...")
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    model = HyenaPositioning(
        input_dim=n_features,
        hidden_dim=384,
        output_dim=2,
        depth=10,
        dropout=0.1,
        num_edge_types=1,
    ).to(device)

    model.load_state_dict(checkpoint["model_state"])
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print()

    # 2. ê° split ë¶„ì„
    splits = ['train', 'val', 'test']
    results = {}

    for split in splits:
        print(f"ğŸ“Š {split.upper()} ë¶„ì„ ì¤‘...")
        jsonl_path = data_dir / f"{split}.jsonl"
        dataset = SlidingWindowDataset(jsonl_path)
        loader = DataLoader(dataset, batch_size=32, shuffle=False)

        results[split] = analyze_split(model, loader, split, device, dataset)
        print(f"âœ… {split.upper()} ì™„ë£Œ: {len(results[split]['errors'])}ê°œ ìƒ˜í”Œ")

    print()

    # 3. Splitë³„ í†µê³„ ë¹„êµ
    print("=" * 80)
    print("ğŸ“Š Splitë³„ ì„±ëŠ¥ ë¹„êµ")
    print("=" * 80)
    print(f"{'Split':<10} {'Samples':<10} {'Mean':<10} {'Median':<10} {'P90':<10} {'P95':<10} {'Max':<10} {'Outliers':<12}")
    print("-" * 80)

    for split in splits:
        errors = results[split]['errors']
        outliers = np.sum(errors >= threshold)
        outlier_pct = outliers / len(errors) * 100

        print(f"{split:<10} {len(errors):<10} {np.mean(errors):<10.3f} {np.median(errors):<10.3f} "
              f"{np.percentile(errors, 90):<10.3f} {np.percentile(errors, 95):<10.3f} "
              f"{np.max(errors):<10.3f} {outliers:<5}({outlier_pct:>4.1f}%)")

    print()

    # 4. ì•„ì›ƒë¼ì´ì–´ ìƒì„¸ ë¶„ì„
    print("=" * 80)
    print(f"ğŸ¯ ì•„ì›ƒë¼ì´ì–´ ìƒì„¸ ë¶„ì„ (â‰¥{threshold}m)")
    print("=" * 80)

    for split in splits:
        errors = results[split]['errors']
        outlier_indices = np.where(errors >= threshold)[0]

        if len(outlier_indices) == 0:
            print(f"\n{split.upper()}: âœ… ì•„ì›ƒë¼ì´ì–´ ì—†ìŒ")
            continue

        print(f"\n{split.upper()}: {len(outlier_indices)}ê°œ ì•„ì›ƒë¼ì´ì–´")
        print("-" * 80)
        print(f"{'Index':<8} {'Error(m)':<12} {'Pred(x,y)':<25} {'Target(x,y)':<25}")
        print("-" * 80)

        # ì˜¤ì°¨ í° ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_outliers = outlier_indices[np.argsort(errors[outlier_indices])[::-1]]

        for idx in sorted_outliers[:10]:  # ìƒìœ„ 10ê°œ
            error = errors[idx]
            pred = results[split]['predictions'][idx]
            target = results[split]['targets'][idx]
            print(f"{idx:<8} {error:<12.3f} ({pred[0]:>6.2f}, {pred[1]:>6.2f})      "
                  f"({target[0]:>6.2f}, {target[1]:>6.2f})")

        if len(sorted_outliers) > 10:
            print(f"... ({len(sorted_outliers)}ê°œ ì¤‘ 10ê°œë§Œ í‘œì‹œ)")

    print()

    # 5. Feature ë¶„ì„ (ì•„ì›ƒë¼ì´ì–´ vs ì •ìƒ)
    print("=" * 80)
    print("ğŸ“ˆ Feature ë¶„ì„ (ì•„ì›ƒë¼ì´ì–´ vs ì •ìƒ)")
    print("=" * 80)

    for split in splits:
        errors = results[split]['errors']
        features = np.array(results[split]['features'])  # [N, 250, n_features]

        outlier_mask = errors >= threshold
        normal_mask = errors < threshold

        if np.sum(outlier_mask) == 0:
            print(f"\n{split.upper()}: ì•„ì›ƒë¼ì´ì–´ ì—†ìŒ")
            continue

        outlier_features = features[outlier_mask]
        normal_features = features[normal_mask]

        print(f"\n{split.upper()}:")
        print(f"  ì •ìƒ ìƒ˜í”Œ feature í‰ê· : {np.mean(normal_features):.6f} (std: {np.std(normal_features):.6f})")
        print(f"  ì•„ì›ƒë¼ì´ì–´ feature í‰ê· : {np.mean(outlier_features):.6f} (std: {np.std(outlier_features):.6f})")
        print(f"  ì°¨ì´: {abs(np.mean(outlier_features) - np.mean(normal_features)):.6f}")

        # ê° feature ì°¨ì›ë³„ë¡œ
        for feat_idx in range(n_features):
            normal_feat = normal_features[:, :, feat_idx]
            outlier_feat = outlier_features[:, :, feat_idx]

            feat_names = ['MagX', 'MagY', 'MagZ', 'Magnitude']
            print(f"  Feature {feat_idx} ({feat_names[feat_idx] if feat_idx < len(feat_names) else feat_idx}):")
            print(f"    ì •ìƒ: mean={np.mean(normal_feat):.6f}, std={np.std(normal_feat):.6f}")
            print(f"    ì•„ì›ƒ: mean={np.mean(outlier_feat):.6f}, std={np.std(outlier_feat):.6f}")

    print()

    # 6. ì‹œê°í™”
    output_dir = Path(__file__).parent / "outputs"
    output_dir.mkdir(exist_ok=True)

    # 6-1. Splitë³„ ì˜¤ì°¨ ë¶„í¬ ë¹„êµ
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    for idx, split in enumerate(splits):
        errors = results[split]['errors']

        # íˆìŠ¤í† ê·¸ë¨
        ax = axes[0, idx]
        ax.hist(errors, bins=50, edgecolor='black', alpha=0.7)
        ax.axvline(threshold, color='red', linestyle='--', label=f'{threshold}m')
        ax.set_xlabel('Error (m)', fontproperties=font_prop)
        ax.set_ylabel('Count', fontproperties=font_prop)
        ax.set_title(f'{split.upper()} ì˜¤ì°¨ ë¶„í¬', fontproperties=font_prop)
        ax.legend(prop=font_prop)
        ax.grid(True, alpha=0.3)

        # CDF
        ax = axes[1, idx]
        sorted_errors = np.sort(errors)
        cdf = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100
        ax.plot(sorted_errors, cdf, linewidth=2)
        ax.axvline(threshold, color='red', linestyle='--')
        ax.axhline(90, color='orange', linestyle='--', label='P90')
        ax.set_xlabel('Error (m)', fontproperties=font_prop)
        ax.set_ylabel('Cumulative %', fontproperties=font_prop)
        ax.set_title(f'{split.upper()} CDF', fontproperties=font_prop)
        ax.legend(prop=font_prop)
        ax.grid(True, alpha=0.3)
        ax.set_xlim(0, min(10, np.max(errors)))

    plt.tight_layout()
    output_path = output_dir / "deep_outlier_analysis.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š ì‹œê°í™” ì €ì¥: {output_path}")

    # 6-2. Split ë¹„êµ ë°•ìŠ¤í”Œë¡¯
    plt.figure(figsize=(12, 6))

    all_errors = [results[s]['errors'] for s in splits]
    plt.boxplot(all_errors, labels=[s.upper() for s in splits])
    plt.axhline(threshold, color='red', linestyle='--', label=f'Outlier threshold ({threshold}m)')
    plt.ylabel('Error (m)', fontproperties=font_prop)
    plt.title('Splitë³„ ì˜¤ì°¨ ë¶„í¬ ë¹„êµ', fontproperties=font_prop)
    plt.legend(prop=font_prop)
    plt.grid(True, alpha=0.3, axis='y')

    output_path2 = output_dir / "split_comparison.png"
    plt.savefig(output_path2, dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š ë¹„êµ ì‹œê°í™” ì €ì¥: {output_path2}")

    print()
    print("=" * 80)
    print("âœ… ì‹¬ì¸µ ë¶„ì„ ì™„ë£Œ!")
    print("=" * 80)

    # 7. ê²°ë¡ 
    print()
    print("=" * 80)
    print("ğŸ“‹ ê²°ë¡ ")
    print("=" * 80)

    # ê³¼ì í•© ì—¬ë¶€
    train_rmse = np.sqrt(np.mean(results['train']['errors']**2))
    val_rmse = np.sqrt(np.mean(results['val']['errors']**2))
    test_rmse = np.sqrt(np.mean(results['test']['errors']**2))

    print(f"  Train RMSE: {train_rmse:.3f}m")
    print(f"  Val RMSE:   {val_rmse:.3f}m")
    print(f"  Test RMSE:  {test_rmse:.3f}m")
    print()

    if abs(val_rmse - test_rmse) < 0.3:
        print("âœ… Val/Test ì„±ëŠ¥ ìœ ì‚¬ â†’ ê³¼ì í•© ì—†ìŒ")
    else:
        print(f"âš ï¸ Val/Test ì°¨ì´ ìˆìŒ: {abs(val_rmse - test_rmse):.3f}m")

    if train_rmse < val_rmse < val_rmse + 1.0:
        print("âœ… Train/Val ì°¨ì´ ì •ìƒ ë²”ìœ„")
    elif train_rmse > val_rmse:
        print("âš ï¸ Trainì´ Valë³´ë‹¤ ë‚˜ì¨ â†’ í•™ìŠµ ë¶€ì¡±?")
    else:
        print(f"âš ï¸ Train/Val ì°¨ì´ í¼: {val_rmse - train_rmse:.3f}m â†’ ê³¼ì í•© ê°€ëŠ¥ì„±")

    print()

    # ì•„ì›ƒë¼ì´ì–´ ë¶„í¬
    for split in splits:
        outliers = np.sum(results[split]['errors'] >= threshold)
        total = len(results[split]['errors'])
        pct = outliers / total * 100
        if pct > 5:
            print(f"âš ï¸ {split.upper()}: ì•„ì›ƒë¼ì´ì–´ {pct:.1f}% ({outliers}/{total}) - ë§ìŒ")
        else:
            print(f"âœ… {split.upper()}: ì•„ì›ƒë¼ì´ì–´ {pct:.1f}% ({outliers}/{total})")

    print()

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Deep outlier analysis across all splits")
    parser.add_argument("--checkpoint", type=str, default="models/hyena_mag4/checkpoints/best.pt")
    parser.add_argument("--data-dir", type=str, default="data/sliding_mag4")
    parser.add_argument("--threshold", type=float, default=5.0)
    parser.add_argument("--cpu", action="store_true")

    args = parser.parse_args()

    device = "cpu" if args.cpu else ("cuda" if torch.cuda.is_available() else "cpu")

    deep_analyze_outliers(
        checkpoint_path=Path(args.checkpoint),
        data_dir=Path(args.data_dir),
        threshold=args.threshold,
        device=device,
    )
