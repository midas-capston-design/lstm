#!/usr/bin/env python3
"""ì•„ì›ƒë¼ì´ì–´ ë¶„ì„: í° ì˜¤ì°¨ë¥¼ ë³´ì´ëŠ” ìƒ˜í”Œ ì°¾ê¸° ë° ë¶„ì„"""
import json
import sys
from pathlib import Path
import torch
import torch.nn as nn
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# í•œê¸€ í°íŠ¸ ì„¤ì • (OS ìë™ ê°ì§€)
def setup_korean_font():
    """ìš´ì˜ì²´ì œì— ë§ëŠ” í•œê¸€ í°íŠ¸ ìë™ ì„¤ì •"""
    import platform

    system = platform.system()

    # ì‹œìŠ¤í…œë³„ í•œê¸€ í°íŠ¸ í›„ë³´
    font_candidates = []

    if system == 'Darwin':  # macOS
        font_candidates = [
            'AppleGothic',
            'Apple SD Gothic Neo',
            'NanumGothic',
        ]
    elif system == 'Windows':
        font_candidates = [
            'Malgun Gothic',
            'NanumGothic',
            'Gulim',
        ]
    else:  # Linux
        font_candidates = [
            'NanumGothic',
            'NanumBarunGothic',
            'UnDotum',
            'DejaVu Sans',
        ]

    # ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    available_fonts = [f.name for f in fm.fontManager.ttflist]

    # í›„ë³´ ì¤‘ ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ í°íŠ¸ ì°¾ê¸°
    for font in font_candidates:
        if font in available_fonts:
            plt.rcParams['font.family'] = font
            plt.rcParams['axes.unicode_minus'] = False
            print(f"âœ… í•œê¸€ í°íŠ¸ ì„¤ì •: {font}")
            return True

    # í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
    print("âš ï¸  í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
    plt.rcParams['axes.unicode_minus'] = False
    return False

# í•œê¸€ í°íŠ¸ ì„¤ì • ì‹œë„
setup_korean_font()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent / "src"))
from model import HyenaPositioning
from torch.utils.data import Dataset, DataLoader

# ì—­ì •ê·œí™”
COORD_CENTER = (-41.0, 0.0)
COORD_SCALE = 50.0

def denormalize_coord(x_norm: float, y_norm: float):
    x = x_norm * COORD_SCALE + COORD_CENTER[0]
    y = y_norm * COORD_SCALE + COORD_CENTER[1]
    return (x, y)

class SlidingWindowDataset(Dataset):
    def __init__(self, jsonl_path: Path):
        self.samples = []
        with jsonl_path.open() as f:
            for line in f:
                self.samples.append(json.loads(line))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        features = torch.tensor(sample["features"], dtype=torch.float32)
        target = torch.tensor(sample["target"], dtype=torch.float32)
        return features, target

def analyze_outliers(
    checkpoint_path: Path,
    data_dir: Path,
    threshold: float = 3.0,  # 3m ì´ìƒì„ ì•„ì›ƒë¼ì´ì–´ë¡œ ì •ì˜
    device: str = "cpu",
):
    """ì•„ì›ƒë¼ì´ì–´ ë¶„ì„"""

    print("=" * 80)
    print("ğŸ” ì•„ì›ƒë¼ì´ì–´ ë¶„ì„")
    print("=" * 80)
    print(f"  Checkpoint: {checkpoint_path}")
    print(f"  Data dir: {data_dir}")
    print(f"  Outlier threshold: {threshold}m")
    print()

    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    meta_path = data_dir / "meta.json"
    with meta_path.open() as f:
        meta = json.load(f)

    n_features = meta["n_features"]

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_path = data_dir / "test.jsonl"
    test_ds = SlidingWindowDataset(test_path)
    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)

    print(f"ğŸ“Š Test samples: {len(test_ds)}ê°œ")
    print()

    # Device ì„¤ì •
    if device == "cuda" and torch.cuda.is_available():
        device = torch.device("cuda")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ Apple Silicon GPU (MPS) ì‚¬ìš©")
    else:
        device = torch.device("cpu")
        print("ğŸ’» CPU ì‚¬ìš©")

    # ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ”„ Checkpoint ë¡œë“œ ì¤‘...")
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    model = HyenaPositioning(
        input_dim=n_features,
        hidden_dim=384,
        output_dim=2,
        depth=10,
        dropout=0.1,
        num_edge_types=1,
    ).to(device)

    model.load_state_dict(checkpoint["model_state"])
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print()

    # ì˜ˆì¸¡ ë° ì˜¤ì°¨ ê³„ì‚°
    print("ğŸ“ˆ ì˜ˆì¸¡ ì¤‘...")
    all_errors = []
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for features, targets in test_loader:
            features = features.to(device)
            targets = targets.to(device)

            edge_ids = torch.zeros(features.size(0), dtype=torch.long, device=device)
            outputs = model(features, edge_ids)
            pred = outputs[:, -1, :]

            pred_np = pred.cpu().numpy()
            target_np = targets.cpu().numpy()

            for i in range(len(pred_np)):
                pred_pos = denormalize_coord(pred_np[i, 0], pred_np[i, 1])
                target_pos = denormalize_coord(target_np[i, 0], target_np[i, 1])

                # Manhattan distance
                dist = abs(pred_pos[0] - target_pos[0]) + abs(pred_pos[1] - target_pos[1])

                all_errors.append(dist)
                all_predictions.append(pred_pos)
                all_targets.append(target_pos)

    all_errors = np.array(all_errors)
    print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ")
    print()

    # ì•„ì›ƒë¼ì´ì–´ ì°¾ê¸°
    outlier_indices = np.where(all_errors >= threshold)[0]
    print("=" * 80)
    print(f"ğŸ¯ ì•„ì›ƒë¼ì´ì–´ ë¶„ì„ (ì˜¤ì°¨ â‰¥ {threshold}m)")
    print("=" * 80)
    print(f"  ì´ ìƒ˜í”Œ ìˆ˜: {len(all_errors)}ê°œ")
    print(f"  ì•„ì›ƒë¼ì´ì–´: {len(outlier_indices)}ê°œ ({len(outlier_indices)/len(all_errors)*100:.1f}%)")
    print()

    if len(outlier_indices) == 0:
        print("âœ… ì•„ì›ƒë¼ì´ì–´ ì—†ìŒ!")
        return

    # ì•„ì›ƒë¼ì´ì–´ ìƒì„¸ ì •ë³´
    print("ğŸ“‹ ì•„ì›ƒë¼ì´ì–´ ìƒì„¸ ì •ë³´:")
    print("-" * 80)
    print(f"{'Index':<8} {'Error(m)':<12} {'Pred(x,y)':<25} {'Target(x,y)':<25}")
    print("-" * 80)

    # ì˜¤ì°¨ í° ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_indices = outlier_indices[np.argsort(all_errors[outlier_indices])[::-1]]

    # ëª¨ë“  outlier ì¶œë ¥
    for idx in sorted_indices:
        error = all_errors[idx]
        pred = all_predictions[idx]
        target = all_targets[idx]
        print(f"{idx:<8} {error:<12.3f} ({pred[0]:>6.2f}, {pred[1]:>6.2f})      ({target[0]:>6.2f}, {target[1]:>6.2f})")

    print()

    # í†µê³„
    print("=" * 80)
    print("ğŸ“Š ì•„ì›ƒë¼ì´ì–´ í†µê³„")
    print("=" * 80)
    outlier_errors = all_errors[outlier_indices]
    print(f"  í‰ê·  ì˜¤ì°¨: {np.mean(outlier_errors):.3f}m")
    print(f"  ì¤‘ì•™ê°’:    {np.median(outlier_errors):.3f}m")
    print(f"  ìµœì†Œ:      {np.min(outlier_errors):.3f}m")
    print(f"  ìµœëŒ€:      {np.max(outlier_errors):.3f}m")
    print()

    # ì „ì²´ ì˜¤ì°¨ ë¶„í¬
    print("=" * 80)
    print("ğŸ“ˆ ì „ì²´ ì˜¤ì°¨ ë¶„í¬")
    print("=" * 80)
    percentiles = [10, 25, 50, 75, 90, 95, 99, 99.5, 100]
    for p in percentiles:
        val = np.percentile(all_errors, p)
        print(f"  P{p:>5}: {val:>8.3f}m")
    print()

    # ì˜¤ì°¨ êµ¬ê°„ë³„ ë¶„í¬
    print("=" * 80)
    print("ğŸ“ ì˜¤ì°¨ êµ¬ê°„ë³„ ë¶„í¬")
    print("=" * 80)
    bins = [0, 1, 2, 3, 5, 10, 20, float('inf')]
    labels = ['0-1m', '1-2m', '2-3m', '3-5m', '5-10m', '10-20m', '>20m']

    for i in range(len(bins)-1):
        count = np.sum((all_errors >= bins[i]) & (all_errors < bins[i+1]))
        pct = count / len(all_errors) * 100
        print(f"  {labels[i]:<10}: {count:>5}ê°œ ({pct:>5.1f}%)")
    print()

    # ì‹œê°í™”
    output_dir = Path(__file__).parent / "outputs"
    output_dir.mkdir(exist_ok=True)

    # 1. ì˜¤ì°¨ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    plt.figure(figsize=(12, 8))

    plt.subplot(2, 2, 1)
    plt.hist(all_errors, bins=50, edgecolor='black', alpha=0.7)
    plt.axvline(threshold, color='red', linestyle='--', label=f'ì„ê³„ê°’ ({threshold}m)')
    plt.xlabel('ì˜¤ì°¨ (m)')
    plt.ylabel('ê°œìˆ˜')
    plt.title('ì „ì²´ ì˜¤ì°¨ ë¶„í¬')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 2. ì˜¤ì°¨ ë¶„í¬ (5m ì´í•˜ë§Œ)
    plt.subplot(2, 2, 2)
    plt.hist(all_errors[all_errors <= 5], bins=50, edgecolor='black', alpha=0.7, color='green')
    plt.xlabel('ì˜¤ì°¨ (m)')
    plt.ylabel('ê°œìˆ˜')
    plt.title(f'ì •ìƒ ë²”ìœ„ (â‰¤{threshold}m)')
    plt.grid(True, alpha=0.3)

    # 3. CDF (ëˆ„ì  ë¶„í¬)
    plt.subplot(2, 2, 3)
    sorted_errors = np.sort(all_errors)
    cdf = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100
    plt.plot(sorted_errors, cdf, linewidth=2)
    plt.axvline(threshold, color='red', linestyle='--', label=f'{threshold}m')
    plt.axhline(90, color='orange', linestyle='--', label='P90')
    plt.axhline(95, color='blue', linestyle='--', label='P95')
    plt.xlabel('ì˜¤ì°¨ (m)')
    plt.ylabel('ëˆ„ì  ë¹„ìœ¨ (%)')
    plt.title('ëˆ„ì  ë¶„í¬ í•¨ìˆ˜ (CDF)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xlim(0, min(10, np.max(all_errors)))

    # 4. Box plot
    plt.subplot(2, 2, 4)
    plt.boxplot([all_errors[all_errors <= 5], outlier_errors],
                tick_labels=[f'ì •ìƒ (â‰¤{threshold}m)', f'ì´ìƒì¹˜ (â‰¥{threshold}m)'])
    plt.ylabel('ì˜¤ì°¨ (m)')
    plt.title('ì˜¤ì°¨ ë¶„í¬ ë¹„êµ')
    plt.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    output_path = output_dir / "outlier_analysis.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š ì‹œê°í™” ì €ì¥: {output_path}")
    print()

    # ì˜ˆì¸¡ vs ì‹¤ì œ ìœ„ì¹˜ (ì•„ì›ƒë¼ì´ì–´ë§Œ)
    if len(outlier_indices) > 0:
        plt.figure(figsize=(10, 10))

        # ì •ìƒ ìƒ˜í”Œ (íšŒìƒ‰)
        normal_indices = np.where(all_errors < threshold)[0]
        normal_preds = [all_predictions[i] for i in normal_indices]
        normal_targets = [all_targets[i] for i in normal_indices]

        if len(normal_preds) > 0:
            normal_pred_x = [p[0] for p in normal_preds]
            normal_pred_y = [p[1] for p in normal_preds]
            normal_target_x = [t[0] for t in normal_targets]
            normal_target_y = [t[1] for t in normal_targets]

            plt.scatter(normal_target_x, normal_target_y, c='lightgray', s=20, alpha=0.3, label='ì •ìƒ (ì‹¤ì œ)')
            plt.scatter(normal_pred_x, normal_pred_y, c='lightblue', s=20, alpha=0.3, label='ì •ìƒ (ì˜ˆì¸¡)')

        # ì•„ì›ƒë¼ì´ì–´ (ë¹¨ê°•)
        outlier_preds = [all_predictions[i] for i in outlier_indices]
        outlier_targets = [all_targets[i] for i in outlier_indices]

        outlier_pred_x = [p[0] for p in outlier_preds]
        outlier_pred_y = [p[1] for p in outlier_preds]
        outlier_target_x = [t[0] for t in outlier_targets]
        outlier_target_y = [t[1] for t in outlier_targets]

        plt.scatter(outlier_target_x, outlier_target_y, c='red', s=100, marker='o',
                   edgecolors='darkred', linewidths=2, label='ì´ìƒì¹˜ (ì‹¤ì œ)', zorder=5)
        plt.scatter(outlier_pred_x, outlier_pred_y, c='orange', s=100, marker='x',
                   linewidths=3, label='ì´ìƒì¹˜ (ì˜ˆì¸¡)', zorder=5)

        # í™”ì‚´í‘œë¡œ ì—°ê²°
        for i in range(len(outlier_indices)):
            plt.arrow(outlier_target_x[i], outlier_target_y[i],
                     outlier_pred_x[i] - outlier_target_x[i],
                     outlier_pred_y[i] - outlier_target_y[i],
                     color='red', alpha=0.3, width=0.1, head_width=0.5,
                     length_includes_head=True, zorder=4)

        plt.xlabel('X (m)')
        plt.ylabel('Y (m)')
        plt.title(f'ì˜ˆì¸¡ vs ì‹¤ì œ ìœ„ì¹˜ (ì´ìƒì¹˜: {len(outlier_indices)}ê°œ)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.axis('equal')

        output_path2 = output_dir / "outlier_positions.png"
        plt.savefig(output_path2, dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š ìœ„ì¹˜ ì‹œê°í™” ì €ì¥: {output_path2}")
        print()

    print("=" * 80)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Analyze outliers in predictions")
    parser.add_argument("--checkpoint", type=str, default="models/hyena_mag4/checkpoints/best.pt")
    parser.add_argument("--data-dir", type=str, default="data/sliding_mag4")
    parser.add_argument("--threshold", type=float, default=3.0, help="Outlier threshold in meters")
    parser.add_argument("--cpu", action="store_true", help="Force CPU usage")

    args = parser.parse_args()

    device = "cpu" if args.cpu else ("cuda" if torch.cuda.is_available() else "cpu")

    analyze_outliers(
        checkpoint_path=Path(args.checkpoint),
        data_dir=Path(args.data_dir),
        threshold=args.threshold,
        device=device,
    )
